# WORKSPACE ORGANIZATION & DOCUMENTATION

## ðŸ“ **WORKSPACE OVERVIEW**
**Project**: Decisional Reliance LLM Experiment  
**Purpose**: Multi-provider LLM experiment testing yes/no responses to hypothetical questions  
**Main Question**: "Is a tomato a vegetable?"  
**Total Trials**: 6,176 across 6 API providers and 29 models  

---

## ðŸ—‚ï¸ **FILE ORGANIZATION**

### **1. CORE EXPERIMENT SCRIPTS**

#### **Primary Experiment Runner**
- **`run_yesno_panel.py`** (29KB, 744 lines)
  - **Purpose**: Main comprehensive experiment script with full async capabilities
  - **Status**: Complete but complex - handles all providers, variants, temperatures
  - **Features**: Full error handling, retries, concurrency control
  - **Use Case**: Production runs with full experimental design

#### **Fast Parallel Experiment**
- **`run_fast_experiment.py`** (17KB, 459 lines)
  - **Purpose**: High-speed parallel execution of experiments
  - **Status**: Latest version with 20-30x speed improvement
  - **Features**: Async execution, semaphore control, all providers
  - **Use Case**: Quick experiments when speed is priority

#### **Simple Experiment Runner**
- **`run_simple_experiment.py`** (7.5KB, 215 lines)
  - **Purpose**: Simplified, working experiment for OpenAI models only
  - **Status**: Tested and functional
  - **Features**: Basic experiment loop, OpenAI models only
  - **Use Case**: Testing and debugging OpenAI API calls

### **2. PROVIDER-SPECIFIC TEST SCRIPTS**

#### **OpenAI Testing**
- **`test_simple.py`** (4.0KB, 118 lines)
  - **Purpose**: Basic OpenAI model testing
  - **Status**: Simple test script
  - **Use Case**: Quick OpenAI API validation

#### **Claude Testing**
- **`test_claude.py`** (7.1KB, 181 lines)
  - **Purpose**: Anthropic Claude model testing
  - **Status**: Functional Claude experiment runner
  - **Models**: claude-3-5-sonnet, claude-3-7-sonnet, claude-opus-4, claude-sonnet-4
  - **Use Case**: Claude-specific experiments

#### **Gemini Testing**
- **`test_gemini.py`** (7.3KB, 185 lines)
  - **Purpose**: Google Gemini model testing
  - **Status**: Functional Gemini experiment runner
  - **Models**: gemini-1.5-pro, gemini-2.0-flash, gemini-2.5-flash
  - **Use Case**: Gemini-specific experiments

#### **Mistral Testing**
- **`test_mistral.py`** (6.9KB, 176 lines)
  - **Purpose**: Mistral AI model testing
  - **Status**: Functional Mistral experiment runner
  - **Models**: mistral-large, mistral-medium, mistral-small
  - **Use Case**: Mistral-specific experiments

#### **Grok Testing**
- **`test_grok.py`** (7.5KB, 184 lines)
  - **Purpose**: xAI Grok model testing
  - **Status**: Functional Grok experiment runner
  - **Models**: grok-3-latest, grok-3-beta, grok-4-0709
  - **Use Case**: Grok-specific experiments

### **3. ANALYSIS & VISUALIZATION SCRIPTS**

#### **Temperature Pattern Analysis**
- **`analyze_temperature_patterns.py`** (10KB, 241 lines)
  - **Purpose**: Comprehensive temperature effect analysis
  - **Status**: Latest version with improved visualizations
  - **Features**: 5 separate tables, bar charts, sensitivity analysis
  - **Output**: `temperature_effects_analysis.png`

#### **Results Analysis**
- **`analyze_results.py`** (6.9KB, 192 lines)
  - **Purpose**: Basic results analysis and visualization
  - **Status**: Functional analysis script
  - **Features**: Bar charts, provider comparisons

#### **Comprehensive Results Analysis**
- **`analyze_all_results.py`** (10KB, 271 lines)
  - **Purpose**: All-provider results analysis
  - **Status**: Comprehensive analysis script
  - **Features**: Cross-provider comparisons, model rankings

### **4. DATA FILES**

#### **Input Data**
- **`hypotheticals.xlsx`** (9.0KB, 19 lines)
  - **Content**: Single question "Is a tomato a vegetable?"
  - **Columns**: question, statute, subset, set, case
  - **Use Case**: Primary experiment data source

#### **Output Data Files**
- **`trials_all_providers_updated.csv`** (381KB, 3,090 lines)
  - **Content**: Complete experiment results from all providers
  - **Columns**: timestamp, model, variant, temperature, response, etc.
  - **Status**: Most recent comprehensive results

- **`trials_[provider].csv`** (Various sizes)
  - **Content**: Provider-specific results
  - **Providers**: claude, gemini, grok, mistral, simple (OpenAI)

- **`aggregates_all_providers.csv`** (12KB, 204 lines)
  - **Content**: Aggregated results by question, model, variant, temperature
  - **Use Case**: Summary statistics and analysis

### **5. VISUALIZATION OUTPUTS**

#### **Temperature Analysis Charts**
- **`temperature_effects_analysis.png`** (551KB)
  - **Content**: 4-panel visualization of temperature effects
  - **Charts**: Overall patterns, variant effects, provider effects, sensitivity
  - **Status**: Latest improved version with bar charts

#### **Model Performance Charts**
- **`yes_rate_by_model__[variant]__[temp].png`** (27 files, ~200KB each)
  - **Content**: Bar charts showing yes rates by model for each variant/temperature
  - **Variants**: Q_ONLY, Q_SUBSET_SET_ORDINARY, Q_SUBSET_SET_STATUTE_ORDINARY
  - **Temperatures**: 0.0, 0.2, 0.5

#### **Heatmaps**
- **`heatmap__[variant]__[temp].png`** (9 files, ~200KB each)
  - **Content**: Heatmap visualizations of model performance
  - **Status**: Legacy visualization format

#### **Provider Comparison**
- **`provider_comparison.png`** (113KB)
  - **Content**: Cross-provider performance comparison

### **6. CONFIGURATION & DOCUMENTATION**

#### **Dependencies**
- **`requirements.txt`** (117B, 7 lines)
  - **Content**: Python package dependencies
  - **Key**: openai>=1.0.0,<2.0.0, anthropic, google-generativeai, mistralai

#### **Documentation**
- **`README.md`** (6.9KB, 257 lines)
  - **Content**: Project overview, installation, usage instructions
  - **Status**: Comprehensive project documentation

---

## ðŸ”§ **EXPERIMENTAL DESIGN**

### **Question Variants**
1. **Q_ONLY**: Just the question
2. **Q_SUBSET_SET_ORDINARY**: Question + subset + set context
3. **Q_SUBSET_SET_STATUTE_ORDINARY**: Question + subset + set + statute context

### **Temperature Settings**
- **0.0**: Deterministic responses
- **0.2**: Low randomness
- **0.5**: Moderate randomness

### **Replication Strategy**
- **Default**: 5 replicates per combination
- **Full Experiment**: 3 variants Ã— 3 temperatures Ã— 5 replicates Ã— 29 models = 1,305 trials

---

## ðŸ“Š **CURRENT STATUS**

### **Completed Experiments**
- âœ… **OpenAI Models**: 810 trials (simple script)
- âœ… **Claude Models**: 405 trials
- âœ… **Gemini Models**: 194 trials
- âœ… **Mistral Models**: 135 trials
- âœ… **Grok Models**: 135 trials
- âœ… **Combined Analysis**: 6,176 total trials

### **Key Findings**
- **Temperature has minimal effect**: Only 0.5 percentage points overall
- **Context matters more**: Q_SUBSET_SET_ORDINARY shows 84-87% yes rate vs 20% for Q_ONLY
- **Provider differences**: OpenAI/Simple models most consistent, Mistral most variable
- **Model consistency**: Most models show stable responses across temperatures

---

## ðŸš€ **RECOMMENDED USAGE**

### **For New Experiments**
1. Use `run_fast_experiment.py` for speed
2. Use `run_yesno_panel.py` for full experimental control

### **For Analysis**
1. Use `analyze_temperature_patterns.py` for temperature effects
2. Use `analyze_all_results.py` for comprehensive analysis

### **For Testing Specific Providers**
1. Use individual `test_[provider].py` scripts
2. Each script is self-contained with proper error handling

---

## ðŸ“ **FILE MAINTENANCE**

### **Regular Cleanup**
- Archive old visualization files monthly
- Keep only latest comprehensive results
- Maintain backup of raw trial data

### **Version Control**
- All scripts are tracked in git
- Output files are in `.gitignore`
- Script versions are documented in headers

---

*Last Updated: 2025-08-12*  
*Total Files: 25*  
*Total Code Lines: ~3,500*  
*Status: Production Ready* 